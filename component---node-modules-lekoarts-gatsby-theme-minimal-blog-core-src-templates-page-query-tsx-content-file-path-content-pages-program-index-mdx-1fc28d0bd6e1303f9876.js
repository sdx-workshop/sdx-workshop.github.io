"use strict";(self.webpackChunkminimal_blog=self.webpackChunkminimal_blog||[]).push([[532],{1873:function(e,t,n){n.d(t,{F:function(){return s},Z:function(){return u}});var a=n(7294),l=n(8733),r=n(795),i=n(6920),o=n(8871);var c=e=>{let{data:{page:t},children:n}=e;return(0,l.tZ)(i.Z,null,(0,l.tZ)(r.X6,{as:"h1",variant:"styles.h1"},t.title),(0,l.tZ)("section",{sx:{my:5,variant:"layout.content"}},n))};const s=e=>{let{data:{page:t}}=e;return(0,l.tZ)(o.Z,{title:t.title,description:t.excerpt})};function u(e){let{...t}=e;return a.createElement(c,t)}},8871:function(e,t,n){var a=n(7294),l=n(1883),r=n(4232);t.Z=e=>{let{title:t="",description:n="",pathname:i="",image:o="",children:c=null,canonicalUrl:s=""}=e;const u=(0,r.Z)(),{siteTitle:m,siteTitleAlt:d,siteUrl:h,siteDescription:g,siteImage:p,author:E,siteLanguage:f}=u,b={title:t?t+" | "+m:d,description:n||g,url:""+h+(i||""),image:""+h+(o||p)};return a.createElement(a.Fragment,null,a.createElement("html",{lang:f}),a.createElement("title",null,b.title),a.createElement("meta",{name:"description",content:b.description}),a.createElement("meta",{name:"image",content:b.image}),a.createElement("meta",{property:"og:title",content:b.title}),a.createElement("meta",{property:"og:url",content:b.url}),a.createElement("meta",{property:"og:description",content:b.description}),a.createElement("meta",{property:"og:image",content:b.image}),a.createElement("meta",{property:"og:type",content:"website"}),a.createElement("meta",{property:"og:image:alt",content:b.description}),a.createElement("meta",{name:"twitter:card",content:"summary_large_image"}),a.createElement("meta",{name:"twitter:title",content:b.title}),a.createElement("meta",{name:"twitter:url",content:b.url}),a.createElement("meta",{name:"twitter:description",content:b.description}),a.createElement("meta",{name:"twitter:image",content:b.image}),a.createElement("meta",{name:"twitter:image:alt",content:b.description}),a.createElement("meta",{name:"twitter:creator",content:E}),a.createElement("meta",{name:"gatsby-theme",content:"@lekoarts/gatsby-theme-minimal-blog"}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"32x32",href:(0,l.withPrefix)("/favicon-32x32.png")}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"16x16",href:(0,l.withPrefix)("/favicon-16x16.png")}),a.createElement("link",{rel:"apple-touch-icon",sizes:"180x180",href:(0,l.withPrefix)("/apple-touch-icon.png")}),s?a.createElement("link",{rel:"canonical",href:s}):null,c)}},7579:function(e,t,n){n.r(t),n.d(t,{Head:function(){return o.F},default:function(){return c}});var a=n(7294),l=n(1151);function r(e){const t=Object.assign({h2:"h2",p:"p",strong:"strong",a:"a",br:"br",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",h3:"h3",blockquote:"blockquote"},(0,l.ah)(),e.components);return a.createElement(a.Fragment,null,a.createElement(t.h2,null,"Important Info"),"\n",a.createElement(t.p,null,a.createElement(t.strong,null,"Location"),": Building 2, Room: 2.1.1, ",a.createElement(t.a,{href:"https://maps.app.goo.gl/VqmEJSmynRcbstZB8"},"Google Maps")," ",a.createElement(t.br),"\n",a.createElement(t.strong,null,"Address"),": Piazza Leonardo da Vinci 32, 20133 Milano ",a.createElement(t.br),"\n",a.createElement(t.strong,null,"Virtual"),": Zoom Webinar (link will be provided via registration mail)"),"\n",a.createElement(t.h2,null,"Schedule"),"\n",a.createElement(t.table,null,a.createElement(t.thead,null,a.createElement(t.tr,null,a.createElement(t.th,null,"Time (CET)"),a.createElement(t.th,null,"Topic"),a.createElement(t.th,null,"Authors"))),a.createElement(t.tbody,null,a.createElement(t.tr,null,a.createElement(t.td,null,"14:00"),a.createElement(t.td,null,"Welcome message"),a.createElement(t.td,null,"Yuki Mitsufuji, Fabian St√∂ter")),a.createElement(t.tr,null,a.createElement(t.td,null,"14:05"),a.createElement(t.td,null,"üèÜ SDX'23 Challenge: Summary & Winner announcements)"),a.createElement(t.td,null,"Giorgio Fabbro, Igor Gadelha, Stefan Uhlich")),a.createElement(t.tr,null,a.createElement(t.td,null,"14:45"),a.createElement(t.td,null,'üéì__Keynote:__ Defining "Source" in Audio Source Separation'),a.createElement(t.td,null,"Gordon Wichern")),a.createElement(t.tr,null,a.createElement(t.td,null,"-"),a.createElement(t.td,null,a.createElement(t.strong,null,"Oral Session 1:")," Insights and Lessons Learned from SDX Participants"),a.createElement(t.td)),a.createElement(t.tr,null,a.createElement(t.td,null,"15:15"),a.createElement(t.td,null,a.createElement(t.a,{href:"/papers/Koo.pdf"},"Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data")),a.createElement(t.td,null,"Junghyun Koo, Yunkee Chae, Chang-Bin Jeon, Kyogu Lee")),a.createElement(t.tr,null,a.createElement(t.td,null,"15:30"),a.createElement(t.td,null,a.createElement(t.a,{href:"/papers/Goswami.pdf"},"Multi-Resolution and Noise Robust Methods for Audio Source Separation")),a.createElement(t.td,null,"Nabarun Goswami, Tatsuya Harada")),a.createElement(t.tr,null,a.createElement(t.td,null,"15:45"),a.createElement(t.td,null,a.createElement(t.a,{href:"/papers/Solovyev.pdf"},"Benchmarks and leaderboards for sound demixing tasks")),a.createElement(t.td,null,"Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva")),a.createElement(t.tr,null,a.createElement(t.td,null,"16:00"),a.createElement(t.td,null,a.createElement(t.a,{href:"/papers/Wang.pdf"},"BS-RoFormer: The SAMI-ByteDance Music Source Separation System for Sound Demixing Challenge 2023")),a.createElement(t.td,null,"Ju-Chiang Wang, Wei-Tsung Lu, Qiuqiang Kong, Yun-Ning Hung")),a.createElement(t.tr,null,a.createElement(t.td,null,"16:15"),a.createElement(t.td,null,a.createElement(t.a,{href:"/papers/Li.pdf"},"Tencent AI Lab‚Äôs CDX 2023 System")),a.createElement(t.td,null,"Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu")),a.createElement(t.tr,null,a.createElement(t.td,null,"16:30"),a.createElement(t.td,null,"‚òïÔ∏è ",a.createElement(t.strong,null,"Coffee Break")," (Virtual: Hangout in Breakout Room)"),a.createElement(t.td)),a.createElement(t.tr,null,a.createElement(t.td,null,"17:00"),a.createElement(t.td,null,"üéì ",a.createElement(t.strong,null,"Keynote:")," Differentiable audio signal processors"),a.createElement(t.td,null,"Christian Steinmetz")),a.createElement(t.tr,null,a.createElement(t.td,null,"-"),a.createElement(t.td,null,a.createElement(t.strong,null,"Oral Session 2:"),"  Audio Source Separation"),a.createElement(t.td)),a.createElement(t.tr,null,a.createElement(t.td,null,"17:30"),a.createElement(t.td,null,a.createElement(t.a,{href:"/papers/Clemens.pdf"},"The Mixology Dataset")),a.createElement(t.td,null,"Michael Clemens")),a.createElement(t.tr,null,a.createElement(t.td,null,"17:45"),a.createElement(t.td,null,a.createElement(t.a,{href:"/papers/Yu.pdf"},"Zero-Shot Duet Singing Voices Separation with Diffusion Models")),a.createElement(t.td,null,"Chin-Yun Yu, Emilian Postolache, Emanuele Rodol√†, Gy√∂rgy Fazekas")),a.createElement(t.tr,null,a.createElement(t.td,null,"18:00"),a.createElement(t.td,null,a.createElement(t.a,{href:"/papers/Dabike.pdf"},"The need for causal, low-latency sound demixing and remixing to improve accessibility")),a.createElement(t.td,null,"Gerardo Roa Dabike, Michael A. Akeroyd, Scott Bannister, Jon Barker, Trevor J. Cox, Bruno Fazenda, Jennifer Firth, Simone Graetzer, Alinka Greasley, Rebecca Vos, William Whitmer")),a.createElement(t.tr,null,a.createElement(t.td,null,"18:15"),a.createElement(t.td,null,a.createElement(t.a,{href:"/papers/Mezza.pdf"},"StemGMD: A Large-Scale Multi-Kit Audio Dataset for Deep Drums Demixing")),a.createElement(t.td,null,"Alessandro Ilic Mezza, Riccardo Giampiccolo, Alberto Bernardini, Augusto Sarti")),a.createElement(t.tr,null,a.createElement(t.td,null,"18:30"),a.createElement(t.td,null,"Panel: Future of SDX Challenge (Things to keep/improve, new tasks, ...)"),a.createElement(t.td,null,"Stefan Uhlich,  Giorgio Fabbro, Fabian St√∂ter, Igor Gadelha")),a.createElement(t.tr,null,a.createElement(t.td,null,"19:00 -"),a.createElement(t.td,null,"Social"),a.createElement(t.td)))),"\n",a.createElement(t.h2,null,"Keynotes"),"\n",a.createElement(t.h3,null,'Gordon Wichern (MERL): Defining "Source" in Audio Source Separation'),"\n",a.createElement(t.p,null,"The cocktail party problem aims at isolating any source of interest within a complex acoustic scene, and has long inspired audio source separation research. In the classical setup, it is generally clear that the source of interest is one speaker among the several simultaneously talking at the party. However, with the explosion of purely data-driven techniques, it is now possible to separate nearly any type of sound from a wide range of signals including non-professional ambient recordings, music, movie soundtracks, and industrial machines. This increase in flexibility has created a new challenge: defining how a user specifies the source of interest. To better embrace this ambiguity, I will first describe how we use hierarchical targets for training source separation networks, where the model learns to separate at multiple levels of granularity, e.g., separate all music from a movie soundtrack in addition to isolating the individual instruments. These hierarchical relationships can be further enforced using hyperbolic representations inside the audio source separation network, enabling novel user interfaces and aiding model explainability. Finally, I will discuss how we incorporate the different meanings for ‚Äúsource‚Äù into source separation model prompts using qualitative audio features, natural language, or example audio clips.\nGordon Wichern"),"\n",a.createElement(t.blockquote,null,"\n",a.createElement(t.p,null,a.createElement(t.strong,null,"Bio"),": Gordon Wichern is a Senior Principal Research Scientist at Mitsubishi Electric Research Laboratories (MERL) in Cambridge, Massachusetts. He received his B.Sc. and M.Sc. degrees from Colorado State University and his Ph.D. from Arizona State University. Prior to joining MERL, he was a member of the research team at iZotope, where he focused on applying novel signal processing and machine learning techniques to music and post-production software, and before that a member of the Technical Staff at MIT Lincoln Laboratory. He is the Chair of the AES Technical Committee on Machine Learning and Artificial Intelligence (TC-MLAI), and a member of the IEEE Audio and Acoustic Signal Processing Technical Committee (AASP-TC). His research interests span the audio signal processing and machine learning fields, with a recent focus on source separation and sound event detection."),"\n"),"\n",a.createElement(t.h3,null,"Christian Steinmetz (QMUL): Differentiable audio signal processors"),"\n",a.createElement(t.p,null,"Large-scale deep generative models have enabled new applications in audio creation and processing. However, these methods often require significant compute which restrict real-time operation, they may introduce artifacts, and they ultimately lack grounding in signal processing operations, limiting controllability and interpretability. Furthermore, many audio production tasks can be addressed with traditional signal processing tools, such as audio effects, but they require expert operation. This motivates differentiable signal processing, which allows for the integration of classic signal processing operations within the gradient-based learning environment, enabling data-driven intelligent operation of these algorithms.\nThis talk will provide an overview of differentiable signal processing techniques, highlighting their benefits and limitations, and offer practical guidance for their implementation. Central to the presentation will be the introduction of DASP ‚Äî Differentiable Audio Signal Processors, a new open-source tool built in PyTorch. DASP offers a range of differentiable audio effects, and we will detail some potential applications, including blind parameter estimation, virtual analog modeling, automatic equalization, and audio production style transfer. To conclude, we'll discuss existing challenges in creating differentiable audio signal processors and suggest potential areas for future exploration."),"\n",a.createElement(t.blockquote,null,"\n",a.createElement(t.p,null,a.createElement(t.strong,null,"Bio"),": Christian Steinmetz is a PhD researcher with the Centre for Digital Music at Queen Mary University of London advised by Joshua Reiss. His research focuses on applications of machine learning for audio signal processing with a focus on high fidelity audio and music production. His work has investigated methods for enhancing audio recordings, automatic and assistive systems for audio engineering, as well as applications of machine learning that augment creativity. He has worked as a research scientist intern at Adobe, Meta, Dolby, and Bose. Christian holds a BS in Electrical Engineering and BA in Audio Technology from Clemson University, as well as an MSc in Sound and Music Computing from the Music Technology Group at Universitat Pompeu Fabra."),"\n"))}var i=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,l.ah)(),e.components);return t?a.createElement(t,e,a.createElement(r,e)):r(e)},o=n(1873);function c(e){return a.createElement(o.Z,e,a.createElement(i,e))}o.Z}}]);
//# sourceMappingURL=component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-page-query-tsx-content-file-path-content-pages-program-index-mdx-1fc28d0bd6e1303f9876.js.map